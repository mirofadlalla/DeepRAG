# ğŸ” DeepRAG â€” Advanced Retrieval-Augmented Generation System

DeepRAG is a **production-inspired Retrieval-Augmented Generation (RAG) system** built completely from scratch.
The goal of this project is to demonstrate a **deep engineering-level understanding** of modern RAG pipelines â€” not a demo or a wrapper around existing frameworks.

This project covers the full lifecycle:
**Ingestion â†’ Chunking â†’ Embeddings â†’ Vector Search â†’ Re-Ranking â†’ Prompt Grounding â†’ Answer Generation**.

---

## ğŸš€ Key Features

- ğŸ“„ **Document Ingestion**
  - Supports TXT and PDF files
  - Extracts text with rich metadata (source, page, language)

- âœ‚ï¸ **Recursive Chunking Engine**
  - Token-based chunking
  - Overlap to preserve context
  - Deterministic chunk IDs (hash-based)

- ğŸ§  **Embeddings**
  - Model: `BAAI/bge-m3`
  - Multilingual (Arabic & English)
  - Normalized embeddings for cosine similarity
  - Embedding cache to avoid recomputation

- âš¡ **Vector Store**
  - FAISS `IndexFlatIP`
  - External mapping between FAISS IDs and chunk IDs
  - Persistent save/load support

- ğŸ” **Semantic Retrieval**
  - Top-K semantic search
  - Metadata-based filtering (language, source)
  - Recall-safe retrieval strategy

- ğŸ¯ **Cross-Encoder Re-Ranking**
  - Model: `BAAI/bge-reranker-base`
  - Re-ranks Top-50 candidates â†’ Top-5
  - High-precision relevance scoring

- ğŸ›¡ **Anti-Hallucination Prompting**
  - Context-only answering
  - Explicit fallback: **"Ù„Ø§ Ø£Ø¹Ù„Ù…"**
  - Mandatory grounding in retrieved chunks
  - Source-aware responses

---

## ğŸ§  System Architecture

Documents (TXT / PDF)
â†“
Chunking Engine
â†“
Embedding Model (BGE-M3)
â†“
FAISS Vector Index + Metadata Store
â†“
Top-K Semantic Retrieval
â†“
Cross-Encoder Re-Ranking
â†“
Prompt Builder (Context Grounding)
â†“
LLM (LLaMA 3)
â†“
Answer + Sources


---

## ğŸ“‚ Project Structure

deep_rag/
â”‚
â”œâ”€â”€ ingestion/
â”‚ â”œâ”€â”€ loader.py # TXT / PDF document loaders
â”‚ â””â”€â”€ chunker.py # Recursive token-based chunking
â”‚
â”œâ”€â”€ embeddings/
â”‚ â””â”€â”€ embedder.py # Embedding engine + caching layer
â”‚
â”œâ”€â”€ vector_store/
â”‚ â””â”€â”€ faiss_index.py # FAISS index + vector-to-chunk mapping
â”‚
â”œâ”€â”€ retrieval/
â”‚ â”œâ”€â”€ retriever.py # Top-K retrieval + metadata filtering
â”‚ â””â”€â”€ reranker.py # Cross-Encoder re-ranking
â”‚
â”œâ”€â”€ llm/
â”‚ â”œâ”€â”€ prompt.py # Strict anti-hallucination prompts
â”‚ â””â”€â”€ generator.py # LLM answer generation
â”‚
â””â”€â”€ DeepRAG.ipynb # End-to-end pipeline notebook


---

## ğŸ§ª Example Use Cases

- ğŸ“š University course material Q&A
- ğŸ¢ Company internal knowledge base
- âš–ï¸ Legal document analysis
- ğŸ§¬ Medical or research paper question answering

---

## ğŸ›¡ Anti-Hallucination Strategy

The system is explicitly designed to **prevent hallucination**:

- LLM is restricted to retrieved context only
- No external or pretrained knowledge allowed
- Explicit instruction to answer **"Ù„Ø§ Ø£Ø¹Ù„Ù…"** when information is missing
- Only Top-5 re-ranked chunks are passed to the LLM
- Answers are always returned with sources

---

## ğŸ“ˆ Why This Project Is Different

âœ” Built without LangChain / LlamaIndex abstractions  
âœ” Full control over every RAG component  
âœ” Clear separation of concerns  
âœ” Production-inspired design decisions  
âœ” Focus on correctness, recall, and grounding  

This project reflects **real RAG engineering**, not prompt-only experimentation.

---

## ğŸ§‘â€ğŸ’» Author

**Omar Yaser**  
Computer Science Student â€” AI & Machine Learning  
Faculty of Computers & Information, Mansoura University  

---

## ğŸ“Œ Notes

- The notebook is used to demonstrate and validate the full pipeline.
- The architecture is intentionally modular to support future extensions:
  - FastAPI deployment
  - Hybrid search
  - Evaluation metrics
  - Large-scale indexing

