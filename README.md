# DeepRAG System â€“ Production-Grade RAG with Drift Monitoring & Benchmarking

A **production-grade Retrieval-Augmented Generation (RAG) system** designed with real-world AI engineering practices in mind. The project goes beyond basic RAG implementations by introducing **systematic benchmarking, drift detection, MLflow tracking, and robustness controls against hallucination**.

This repository represents an end-to-end **LLMOps-ready RAG architecture**, suitable for research, applied AI, and enterprise use cases.

---

## ğŸš€ Key Highlights

* End-to-end modular RAG pipeline (Embedding â†’ Retrieval â†’ Reranking â†’ Generation)
* FAISS IVF indexing for scalable, low-latency semantic search
* Cross-Encoder reranking for high-precision retrieval
* Comprehensive evaluation & benchmarking framework
* Continuous **Drift Detection** (Query, Retrieval, Hallucination)
* MLflow-based experiment tracking and monitoring
* Strong hallucination mitigation via prompt constraints and numeric grounding
* Production-oriented error handling and serialization safety

---

## ğŸ§  System Architecture (High-Level)

**Pipeline Flow:**

1. Document ingestion & preprocessing
2. Semantic chunking
3. Dense embedding (BAAI/bge-m3)
4. FAISS IVF retrieval
5. BM25 + metadata filtering
6. Multi-retriever fusion
7. Cross-Encoder reranking
8. LLM answer generation
9. Evaluation, logging, and drift monitoring

---

## ğŸ“ Project Structure

```
Buliding Rag System/
â”œâ”€â”€ benchmark.py                    # Full offline benchmark runner
â”œâ”€â”€ drift_monitoring_example.py     # Integrated benchmark + drift monitoring
â”œâ”€â”€ drift_detection.py              # Drift detection logic
â”œâ”€â”€ drift_dashboard.py              # Monitoring dashboard utilities
â”œâ”€â”€ evaluation_dataset.json         # Evaluation questions & references
â”œâ”€â”€ evaluation_results/             # Per-question metric artifacts
â”œâ”€â”€ benchmark_reports/              # Aggregated benchmark outputs
â”‚   â”œâ”€â”€ full_results.json
â”‚   â”œâ”€â”€ full_results.csv
â”‚   â”œâ”€â”€ all_metrics.json
â”‚   â”œâ”€â”€ drift_report.txt
â”‚   â””â”€â”€ metrics_pickles/
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ pipeline.py                 # Main RAG orchestration layer
â”œâ”€â”€ steps/
â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”œâ”€â”€ chunking_engine.py
â”‚   â”œâ”€â”€ embedder.py
â”‚   â”œâ”€â”€ faiss_index.py
â”‚   â”œâ”€â”€ bm25_index.py
â”‚   â”œâ”€â”€ retrievalfiltering.py
â”‚   â”œâ”€â”€ fusion.py
â”‚   â”œâ”€â”€ Cross_Encoder.py
â”‚   â”œâ”€â”€ query_expansion.py
â”‚   â”œâ”€â”€ prompt_engineering.py
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ metrics.py                  # Evaluation metric implementations
â””â”€â”€ data/
    â””â”€â”€ processed/
        â”œâ”€â”€ faiss.index
        â”œâ”€â”€ faiss_mapping.json
        â””â”€â”€ embeddings/
```

---

## ğŸ“Š Evaluation & Benchmarking

The system includes a **research-grade evaluation framework**.

### Retrieval Metrics

* **MRR (Mean Reciprocal Rank)**
* **Precision@K**
* **Recall@K**

### Answer Quality Metrics

* **Answer Relevance** (binary)
* **Hallucination Score** (0â€“1)
* **Jaccard Similarity**

### Performance Metrics

* FAISS search latency
* End-to-end pipeline time

All metrics are:

* Logged per question
* Aggregated across datasets
* Persisted as JSON / CSV / Pickle
* Tracked in **MLflow**

---

## ğŸ“ˆ Drift Detection System

A core differentiator of this project.

### Drift Types Monitored

* **Query Drift** â€“ semantic change in incoming queries
* **Retrieval Drift** â€“ degradation in retrieval quality
* **Hallucination Drift** â€“ increased unsupported generation

### Features

* Batch-based drift analysis
* Daily scheduled checks
* Automatic drift reports
* MLflow logging of drift signals
* Stability trend tracking

This enables **long-term reliability monitoring**, not just one-off evaluation.

---

## ğŸ§ª MLflow Integration

MLflow is used as a first-class component:

* Experiment-level tracking
* Per-run metrics & parameters
* Artifact storage (reports, pickles)
* Safe handling of nested runs
* Robust serialization of pipeline outputs

The system avoids common MLflow pitfalls such as:

* Active run conflicts
* Non-serializable object logging

---

## ğŸ›¡ï¸ Hallucination Mitigation

During evaluation, it was observed that **surface-level abstraction** (e.g., translating numeric values into natural language) could trigger hallucination flags.

### Solution Implemented

* Enforced **literal numeric & unit preservation** in generation prompts
* Grounded answer generation strictly to retrieved context

This significantly reduced false hallucination detection while preserving correctness.

---

## âš™ï¸ Configuration

### Embedding & Retrieval

```python
MODEL_NAME = "BAAI/bge-m3"
TOP_K = 5
```

### FAISS IVF

```python
nlist = 100
nprobe = 10
use_ivf = True
```

### Drift Monitoring

```python
BATCH_SIZE = 50
DAILY_CHECK_HOUR = 2
```

---

## â–¶ï¸ Usage

### Run Full Benchmark

```bash
python benchmark.py
```

### Run Benchmark + Drift Monitoring

```bash
python drift_monitoring_example.py
```

### View MLflow Dashboard

```bash
mlflow ui
```

Access at: `http://localhost:5000`

---

## ğŸ“¦ Outputs

### Benchmark Reports

* `full_results.json`
* `full_results.csv`
* `all_metrics.json`
* `drift_report.txt`

### Per-Question Artifacts

* Retrieval metrics
* Hallucination scores
* Answer relevance
* Jaccard similarity

---

## ğŸ§© Design Decisions (Rationale)

* **BGE-M3**: Strong multilingual + retrieval performance
* **FAISS IVF**: Scales efficiently for large corpora
* **Cross-Encoder**: Improves ranking precision beyond dense retrieval
* **MLflow**: Industry-standard experiment tracking
* **Drift Monitoring**: Required for real production LLM systems

---

## ğŸ”® Future Improvements

* Online (real-time) drift detection
* GPU-accelerated FAISS
* Cost & latency monitoring
* Human feedback integration
* Adaptive retriever selection

---

## ğŸ‘¨â€ğŸ’» Author

**Omar Yaser**
AI Engineer â€“ RAG, LLMs, and MLOps

---

## âœ… Project Status

**Production-Ready**

* Stable pipeline
* Verified benchmarks
* Robust drift monitoring
* Clean MLflow integration
* Extensive documentation

This project reflects real-world AI engineering practices rather than academic prototypes.
